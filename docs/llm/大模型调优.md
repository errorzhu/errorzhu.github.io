# 大模型微调

## 一 微调的概念

大模型训练的步骤

![llm_train_flow.png](D:\project\errorzhu.github.io\docs\images\wiki\llm_train_flow.png)

- **预训练（Pretraining）**：基于海量语料进行Transformer Decoder架构的自回归预训练，拟合语料序列的条件概率分布P(wi|wi,...,wi−1)，从而压缩信息，最终学到一个具备长上下文建模能力的超大规模神经语言模型，即LLM
- **有监督微调（Supervised Finetuning）**：基于高质量的指令数据(用户输入的提示词 + 对应的理想输出结果)微调LLM，从而得到有监督微调模型（SFT模型）。SFT模型将具备初步的指令理解能力和上下文理解能力（预训练得到的LLM在指令微调的过程中被引导如何使用其学到的知识）  
- **奖励建模（Reward Modeling）**：奖励阶段试图构建一个文本质量对比模型（相当于一个Critor）。对同一个提示词，它将对SFT模型给出的多个不同输出的质量做排序。奖励模型可通过二分类模型，对输入的两个结果之间的优劣进行判断。
- **强化学习（Reinforcement Learning）**：强化学习阶段将根据给定的提示词样本数据，利用在前一阶段训练的奖励模型，给出SFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。强化学习微调将在SFT模型基础上，它将使LLM生成的结果文本能获得更高的奖励。

模型微调（Fine-tuning）是指在已有的大规模预训练语言模型（如GPT-3、GPT-4、BERT等）基础上，针对特定任务或领域进行的二次训练过程。预训练模型通常在大规模无标注文本数据上进行训练，以学习语言的通用表示和规律。微调则是利用针对性的小规模、有标签的数据集，调整模型参数以使其更好地适应并精准完成特定任务，如文本分类、问答、机器翻译、情感分析等。

todo（原理，作用展开，结合业务输出一个电力行业的案例）

## 二 微调分类

### 1. 全参数微调（Full-Parameter Fine-tuning）

**方案描述**：对模型所有参数进行微调，适用于数据充足、计算资源丰富的场景，能最大化模型性能，但计算和存储成本高。

**适用场景**：

- 需要大幅调整模型行为（如特定领域任务）。
- 有大量标注数据和高性能计算资源。

**技术要点**：

- 使用监督学习（Supervised Fine-tuning, SFT），基于任务特定的标注数据集更新所有权重。
- 需防止过拟合，通常结合正则化（如Dropout、权重衰减）。
- 优化器常用AdamW，学习率较预训练低（如1e-5~1e-4）。

**优缺点**：

- 优点：性能提升显著，适应性强。
- 缺点：显存需求大，存储整个模型副本。

### 2. 参数高效微调（Parameter-Efficient Fine-tuning, PEFT）

![peft.png](D:\project\errorzhu.github.io\docs\images\wiki\peft.png)

**方案描述**：仅更新少量参数或附加模块，降低计算和存储需求，适合资源受限场景。

**优缺点**：

- 优点：显存需求低，存储成本小，适合大规模模型。
- 缺点：性能可能略逊于全参数微调，需调优超参数。

**子方案**：

## （1） LoRA (Low-Rank Adaptation)

**基本原理**：
LoRA通过添加低秩矩阵来近似权重更新，而不是直接微调所有参数。它对预训练权重矩阵 W 的更新表示为低秩分解：ΔW = A × B，其中A和B是低秩矩阵。

**优点**：

- 参数效率高：仅需训练约0.1%-1%的参数
- 内存占用低：原始预训练权重保持冻结状态
- 模型切换方便：不同任务的LoRA模块可以快速切换
- 训练速度快：显著减少了训练时间和计算资源需求

**缺点**：

- 表现可能略低于全参数微调
- 需要为每层选择合适的秩(rank)
- 不是所有层都同样适合LoRA

**适用场景**：

- 计算资源有限的环境
- 需要快速适应多个下游任务
- 大型语言模型的领域适应

**实施流程**：

1. 冻结预训练模型的全部参数
2. 为需要训练的权重矩阵(通常是注意力层和前馈层)添加LoRA模块
3. 设定低秩参数r(通常为4-64)
4. 仅训练LoRA参数(A和B矩阵)
5. 推理时可以将LoRA权重合并到原始权重或保持独立

## （2）QLoRA (Quantized Low-Rank Adaptation)

**基本原理**：
QLoRA结合了模型量化和LoRA技术，将基础模型量化为4-bit精度，同时使用LoRA进行微调。

**优点**：

- 大幅降低内存需求：可以在单个GPU上微调非常大的模型
- 保持接近全精度微调的性能
- 与LoRA相比进一步降低了资源需求
- 采用特殊的量化技术(如NormalFloat)减少量化误差

**缺点**：

- 推理速度可能略慢于原始模型
- 需要特殊的量化和反量化操作
- 设置比标准LoRA更复杂

**适用场景**：

- 极其有限的硬件资源环境
- 需要微调大型模型(如65B+参数)但GPU内存有限
- 注重微调效率而非推理速度的应用

**实施流程**：

1. 将预训练模型量化为4-bit精度(通常使用NF4格式)
2. 实现基于4-bit量化的分页优化器
3. 添加LoRA适配器层
4. 执行微调(仅LoRA参数更新)
5. 推理时可选择性地将模型转回更高精度

## （3）Adapter Tuning (适配器微调)

**基本原理**：
在预训练模型的层与层之间插入小型可训练模块(适配器)，通常包含降维层、非线性激活函数和升维层。原始参数保持冻结。

**优点**：

- 模块化设计：易于添加和移除适配器
- 参数效率高：通常仅增加1-4%的参数
- 可并行化：不同任务的适配器可独立训练
- 适应性强：可以有选择地放置在网络的不同部分

**缺点**：

- 增加了模型的深度和推理延迟
- 架构改变较大：需要修改原始模型架构
- 较LoRA实现复杂：需要适配不同类型的层

**适用场景**：

- 多任务学习环境
- 需要长期维护多个专业化模型变体
- 特别注重模块化和任务隔离的应用

**实施流程**：

1. 冻结预训练模型参数
2. 在关键层后(通常是自注意力层和前馈层后)插入适配器模块
3. 设计适配器架构(通常是降维-激活-升维结构)
4. 仅训练适配器参数
5. 推理时可选择性地启用或禁用适配器

## （4）Prompt Tuning (提示调优)

**基本原理**：
在输入序列前添加一组可学习的连续嵌入向量("软提示")，而不修改模型参数。这些嵌入向量通过梯度下降优化。

**优点**：

- 极其参数高效：每个任务仅需几百个参数
- 模型架构完全不变
- 多任务切换成本极低：仅需替换提示向量
- 随着模型规模增大效果逐渐接近全参数微调

**缺点**：

- 在较小模型上效果有限
- 训练收敛可能较慢
- 对不同任务的泛化能力相对较弱
- 通常需要较长的提示(数十至数百个token)

**适用场景**：

- 超大规模语言模型(100B+参数)
- 服务提供商需要为多个用户定制模型
- 存储空间极其有限的环境
- API访问条件下的模型优化

**实施流程**：

1. 初始化可学习的提示向量(通常基于词汇表随机选择或特定模式)
2. 将提示向量连接到输入序列前
3. 冻结所有模型参数，仅训练提示向量
4. 对每个任务分别存储优化后的提示向量
5. 推理时使用相应任务的提示向量

## （5）Prefix Tuning (前缀微调)

**基本原理**：
在模型内部多个层的键值向量前添加可训练的前缀向量，而不仅仅是在输入层。这些前缀直接影响模型的每一层注意力计算。

**优点**：

- 比Prompt Tuning更强的表现力：影响模型的多层计算
- 参数数量适中：通常约为0.1%-0.5%的原模型参数
- 适用于生成任务：特别适合文本生成、摘要等任务
- 对较小模型也有良好效果

**缺点**：

- 实现复杂度高：需要修改模型的每一层注意力机制
- 训练不如LoRA稳定
- 内存效率低于Prompt Tuning
- 调参相对困难

**适用场景**：

- 文本生成、摘要、对话系统等生成式任务
- 需要更强控制力的应用场景
- 中小规模模型的优化

**实施流程**：

1. 设计前缀参数矩阵(通常通过MLP参数化)
2. 在每个Transformer层的自注意力模块中插入前缀向量
3. 冻结原始模型参数，仅训练前缀参数
4. 对不同任务保存不同的前缀参数
5. 推理时将相应前缀加载到各层

## （6）P-Tuning (提示调优)

**基本原理**：
使用少量可训练的连续提示嵌入，并通过小型神经网络(通常是LSTM或MLP)生成这些嵌入，而不是直接优化它们。

**优点**：

- 比直接Prompt Tuning更稳定：使用神经网络参数化提高稳定性
- 可以插入到输入序列的不同位置(不仅限于开头)
- 参数量很少：通常只有数千个参数
- 特别适合知识密集型和分类任务

**缺点**：

- 不如P-Tuning v2和LoRA在广泛任务上表现好
- 优化相对复杂：需要调整网络结构
- 实现略微复杂：需要额外的神经网络组件
- 在小模型上效果有限

**适用场景**：

- 知识密集型任务(如实体关系抽取、事实回答)
- 分类任务
- 需要插入模板的场景(如在特定位置插入可训练标记)

**实施流程**：

1. 设计提示模板，确定可训练提示token的位置
2. 构建编码网络(如双向LSTM)来参数化可训练提示
3. 初始化提示嵌入
4. 训练编码网络和提示嵌入，保持主模型冻结
5. 推理时使用编码网络生成最终提示嵌入

## 总结比较

| 方法            | 参数效率        | 内存使用 | 实现复杂度 | 最佳适用场景       | 特点                   |
| ------------- | ----------- | ---- | ----- | ------------ | -------------------- |
| LoRA          | 高(0.1-1%)   | 低    | 中等    | 通用场景，资源受限环境  | 低秩矩阵分解，灵活性高          |
| QLoRA         | 极高          | 极低   | 高     | 极限资源条件，超大模型  | 结合量化技术，可在单GPU上微调超大模型 |
| Adapter       | 中(1-4%)     | 中    | 高     | 多任务学习，模块化需求  | 架构明确，增加网络深度          |
| Prompt Tuning | 极高(0.01%)   | 极低   | 低     | 超大模型，API访问场景 | 仅修改输入，实现简单，大模型效果好    |
| Prefix Tuning | 高(0.1-0.5%) | 中低   | 高     | 生成任务，需要强控制力  | 修改多层注意力，表现力强         |
| P-Tuning      | 极高(约0.01%)  | 极低   | 中     | 知识密集型任务，分类任务 | 通过神经网络优化提示，定位精确      |

每种方法都有其独特优势和适用场景，选择时应根据具体任务需求、可用资源和性能要求进行综合考虑。

## 混合方案（电力业务的案例）（效果对比文本）

## <1>. Microsoft UniPELT (Universal Parameter-Efficient Language Model Tuning)

**混合技术**：Adapter + LoRA + Prefix Tuning + BitFit

**实现细节**：

- 在同一模型中同时使用多种PEFT方法
- 为Transformer的不同组件应用不同的微调方案：
  - 自注意力查询/键/值矩阵使用LoRA
  - 前馈网络中插入Adapter
  - 每层添加可学习的Prefix向量
  - 应用BitFit选择性更新关键偏置参数
- 使用门控机制动态调整各组件的重要性

**应用场景**：

- GLUE基准测试(自然语言理解)
- SuperGLUE(复杂推理任务)
- SQuAD(问答任务)

**效果**：在多任务迁移学习中接近全参数微调的性能，但仅使用约2%的可训练参数

## <2>. LLaMA-Adapter V2

**混合技术**：LoRA + 零初始化Adapter + 提示微调

**实现细节**：

- 在自注意力层使用LoRA降低秩矩阵
- 在每个Transformer块后添加零初始化的并行适配器
- 在输入序列前添加任务特定的可学习提示标记
- 引入了"gated adaptation"门控适应机制，调节原始模型与适配器间的信息流

**应用场景**：

- 指令遵循任务
- 多模态理解(视觉-语言任务)
- 复杂推理任务

**效果**：仅使用1%的训练参数，在多模态任务上实现了接近全参数微调的性能，同时保持了极高的推理效率

## <3>. QA-LoRA (Query-Adapter LoRA)

**混合技术**：QLoRA + Adapter + MoE(专家混合)

**实现细节**：

- 将基础模型量化为4位精度
- 在注意力层应用LoRA进行低秩更新
- 在前馈网络后添加适配器模块
- 对不同类型的查询使用不同的专家适配器(基于混合专家模型思想)
- 使用路由机制自动选择最适合当前查询的适配器组合

**应用场景**：

- 开放域问答
- 多语言自然语言理解
- 医疗和法律等垂直领域应用

**效果**：在多领域问答任务上比单独使用QLoRA提高了5-8%的准确率，同时保持了内存效率

## 三 微调框架

- https://github.com/hiyouga/LLaMA-Factory

- [PEFT - Hugging Face 机器学习平台](https://hugging-face.cn/docs/peft/index)

- [GitHub - unslothai/unsloth: Finetune Qwen3, Llama 4, TTS, DeepSeek-R1 &amp; Gemma 3 LLMs 2x faster with 70% less memory! 🦥](https://github.com/unslothai/unsloth)

    

## 四 参考资料

- Hugging Face 微调指南：[Fine-tuning](https://huggingface.co/docs/transformers/training)
- Hugging Face PEFT 文档：[PEFT](https://huggingface.co/docs/peft)
- [使用微调定制属于自己的大模型模型效果差？输出不够稳定？本文将介绍大模型微调原理与实践以及所适用的场景。试试用微调定制属于 - 掘金](https://juejin.cn/post/7321558573518241818)
- https://github.com/luhengshiwo/LLMForEverybody
- https://github.com/liguodongiot/llm-action
- https://zhuanlan.zhihu.com/p/30057718169
- https://zhuanlan.zhihu.com/p/17628689019
- [DeepSeek大模型微调实战（理论篇） - OSCHINA - 中文开源技术交流社区](https://my.oschina.net/u/8066678/blog/17270259)
- [手把手教学，DeepSeek-R1微调全流程拆解 - 雨梦山人 - 博客园](https://www.cnblogs.com/shanren/p/18707513)
